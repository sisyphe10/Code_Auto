import time
import schedule
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
import os
import csv
import yfinance as yf
import warnings
import FinanceDataReader as fdr  # [ë³€ê²½] pykrx ëŒ€ì‹  fdr ì‚¬ìš©
import pandas as pd

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.simplefilter(action='ignore', category=FutureWarning)

# === ìƒìˆ˜ ì •ì˜ ===
CSV_FILE = 'dataset.csv'

# DRAM ì œí’ˆëª…
TARGET_DRAM_ITEMS = {
    'DDR5 16G (2Gx8) 4800/5600': 'DDR5 16G (2Gx8) 4800/5600',
    'DDR4 16Gb (1Gx16)3200': 'DDR4 16Gb (1Gx16)3200',
    'DDR4 16Gb (2Gx8)3200': 'DDR4 16Gb (2Gx8)3200',
    'DDR4 8Gb (1Gx8) 3200': 'DDR4 8Gb (1Gx8) 3200',
    'DDR4 8Gb (512Mx16) 3200': 'DDR4 8Gb (512Mx16) 3200'
}

# NAND ì œí’ˆëª…
TARGET_NAND_ITEMS = {
    'SLC 2Gb 256MBx8': 'SLC 2Gb 256MBx8',
    'SLC 1Gb 128MBx8': 'SLC 1Gb 128MBx8',
    'MLC 64Gb 8GBx8': 'MLC 64Gb 8GBx8',
    'MLC 32Gb 4GBx8': 'MLC 32Gb 4GBx8'
}

# yfinance í‹°ì»¤ ëª©ë¡
YFINANCE_TICKERS = {
    # --- ì•”í˜¸í™”í ---
    'Bitcoin': {'ticker': 'BTC-USD', 'type': 'CRYPTO'},
    'Ethereum': {'ticker': 'ETH-USD', 'type': 'CRYPTO'},
    'Binance Coin': {'ticker': 'BNB-USD', 'type': 'CRYPTO'},

    # --- ì›ìì¬ ---
    'WTI Crude Oil': {'ticker': 'CL=F', 'type': 'COMMODITY'},
    'Brent Crude Oil': {'ticker': 'BZ=F', 'type': 'COMMODITY'},
    'Natural Gas': {'ticker': 'NG=F', 'type': 'COMMODITY'},
    'Gold': {'ticker': 'GC=F', 'type': 'COMMODITY'},
    'Silver': {'ticker': 'SI=F', 'type': 'COMMODITY'},
    'Copper': {'ticker': 'HG=F', 'type': 'COMMODITY'},
    'Uranium ETF (URA)': {'ticker': 'URA', 'type': 'COMMODITY'},

    # --- ì§€ìˆ˜ ë° ê¸ˆë¦¬ ---
    'VIX Index': {'ticker': '^VIX', 'type': 'INDEX'},
    'US 10 Year Treasury Yield': {'ticker': '^TNX', 'type': 'INTEREST_RATE'},

    # --- í™˜ìœ¨ (FX) ---
    'Dollar Index (DXY)': {'ticker': 'DX-Y.NYB', 'type': 'FX'},
    'KRW/USD': {'ticker': 'KRW=X', 'type': 'FX'},
    'JPY/USD': {'ticker': 'JPY=X', 'type': 'FX'},
    'CNY/USD': {'ticker': 'CNY=X', 'type': 'FX'},
    'TWD/USD': {'ticker': 'TWD=X', 'type': 'FX'},
    'EUR/USD': {'ticker': 'EUR=X', 'type': 'FX'},
}


# === ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ===
def setup_csv():
    """CSV íŒŒì¼ ì´ˆê¸° ì„¤ì •"""
    if not os.path.exists(CSV_FILE):
        with open(CSV_FILE, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['ë‚ ì§œ', 'ì œí’ˆëª…', 'ê°€ê²©', 'ë°ì´í„° íƒ€ì…'])
        print(f"âœ… CSV íŒŒì¼ ìƒì„± ì™„ë£Œ: {CSV_FILE}")
    else:
        print(f"âœ… ê¸°ì¡´ CSV íŒŒì¼ ì‚¬ìš©: {CSV_FILE}")


def setup_driver(headless=True):
    """Selenium ì›¹ë“œë¼ì´ë²„ ì„¤ì •"""
    chrome_options = Options()
    if headless:
        chrome_options.add_argument('--headless')

    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0')

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver


def save_to_csv(data):
    """ì¤‘ë³µ ë°©ì§€ ê¸°ëŠ¥ì´ ê°•í™”ëœ CSV ì €ì¥ (ë°°ì¹˜ ë‚´ ì¤‘ë³µê¹Œì§€ ì œê±°)"""
    try:
        existing_keys = set()
        if os.path.exists(CSV_FILE):
            with open(CSV_FILE, 'r', encoding='utf-8-sig') as f:
                reader = csv.reader(f)
                next(reader, None)
                for row in reader:
                    if len(row) >= 2:
                        key = (row[0], row[1])
                        existing_keys.add(key)

        new_data = []
        current_batch_keys = set()

        for row in data:
            current_key = (row[0], row[1])
            if current_key not in existing_keys and current_key not in current_batch_keys:
                new_data.append(row)
                current_batch_keys.add(current_key)

        if new_data:
            with open(CSV_FILE, 'a', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                writer.writerows(new_data)
            print(f"âœ… {len(new_data)}ê±´ ì €ì¥ ì™„ë£Œ (ì¤‘ë³µ ì œì™¸ë¨)")
            return True
        else:
            print("ğŸ’¡ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì¤‘ë³µ)")
            return True

    except Exception as e:
        print(f"\nâŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False


def get_last_scfi_date():
    try:
        if not os.path.exists(CSV_FILE): return None
        last_date = None
        with open(CSV_FILE, 'r', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            next(reader, None)
            for row in reader:
                if len(row) >= 4 and row[3] == 'OCEAN_FREIGHT' and 'SCFI' in row[1]:
                    last_date = row[0]
        return last_date
    except:
        return None


# ==========================================
# 1. [KRX] í•œêµ­ ì§€ìˆ˜/ì‹œì´/ì¢…ëª©ìˆ˜ (FinanceDataReader ì‚¬ìš©)
# ==========================================
def crawl_krx_indices():
    """FinanceDataReaderë¥¼ ì‚¬ìš©í•˜ì—¬ KOSPI, KOSDAQì˜ ì§€ìˆ˜, ì‹œê°€ì´ì•¡, ì¢…ëª©ìˆ˜ë¥¼ ìˆ˜ì§‘"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ‡°ğŸ‡· KRX ì¢…í•© ë°ì´í„°(fdr) í¬ë¡¤ë§ ì‹œì‘")
    print(f"{'=' * 60}")

    collected_data = []
    today_str = datetime.now().strftime("%Y-%m-%d")

    try:
        # 1. ì „ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        df_master = fdr.StockListing('KRX')
        
        # ì»¬ëŸ¼ëª… í‘œì¤€í™”
        col_map = {'MarketCap': 'Marcap', 'MarCap': 'Marcap', 'Name': 'Name', 'Code': 'Code', 'Market': 'Market'}
        df_master = df_master.rename(columns={k: v for k, v in col_map.items() if k in df_master.columns})

        # 2. ì‹œì¥ë³„ ë¶„ì„
        target_markets = ['KOSPI', 'KOSDAQ']
        
        for market in target_markets:
            try:
                mkt_df = df_master[df_master['Market'] == market]
                
                # A. ìˆœìˆ˜ ìƒì¥ ì¢…ëª©ìˆ˜
                real_stocks = mkt_df[
                    (mkt_df['Code'].str.endswith('0')) & 
                    (~mkt_df['Name'].str.contains('ìŠ¤íŒ©')) & 
                    (~mkt_df['Name'].str.contains('ë¦¬ì¸ '))
                ]
                count = len(real_stocks)
                collected_data.append((today_str, f"{market} ìƒì¥ì¢…ëª©ìˆ˜", count, 'INDEX_KR'))
                print(f"âœ“ {market} ìˆœìˆ˜ ì¢…ëª©ìˆ˜: {count}ê°œ")

                # B. ì‹œê°€ì´ì•¡ í•©ê³„
                if 'Marcap' in mkt_df.columns:
                    total_cap = mkt_df['Marcap'].sum()
                    collected_data.append((today_str, f"{market} ì‹œê°€ì´ì•¡", float(total_cap), 'INDEX_KR'))
                    print(f"âœ“ {market} ì‹œê°€ì´ì•¡ í•©ê³„ ì§‘ê³„ ì™„ë£Œ")

            except Exception as e:
                print(f"âš ï¸ {market} ì¢…ëª© ë¶„ì„ ì‹¤íŒ¨: {e}")

        # 3. ì§€ìˆ˜ ê°€ê²©
        index_map = {'KOSPI': 'KS11', 'KOSDAQ': 'KQ11', 'KOSPI 200': 'KS200'}

        for name, symbol in index_map.items():
            try:
                df_idx = fdr.DataReader(symbol, today_str, today_str)
                if df_idx.empty:
                    prev_date = (datetime.now() - timedelta(days=5)).strftime("%Y-%m-%d")
                    df_idx = fdr.DataReader(symbol, prev_date)
                
                if not df_idx.empty:
                    last_row = df_idx.iloc[-1]
                    price = float(last_row['Close'])
                    date_val = last_row.name.strftime("%Y-%m-%d")
                    collected_data.append((date_val, name, price, 'INDEX_KR'))
                    print(f"âœ“ {name} ì§€ìˆ˜: {price:,.2f}")
            except Exception as e:
                print(f"âš ï¸ {name} ì§€ìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")

    except Exception as e:
        print(f"âŒ KRX ë°ì´í„° ìˆ˜ì§‘ ì „ì²´ ì‹¤íŒ¨: {e}")

    if collected_data:
        save_to_csv(collected_data)


# ==========================================
# 2. [US] ë¯¸êµ­ ì§€ìˆ˜/PER/PBR (yfinance)
# ==========================================
def crawl_us_indices():
    """ë¯¸êµ­ ì§€ìˆ˜ ë° PER/PBR ìˆ˜ì§‘"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì§€ìˆ˜/PER/PBR í¬ë¡¤ë§ ì‹œì‘ (yfinance)")
    print(f"{'=' * 60}")

    collected_data = []

    targets = {
        "S&P 500": {"idx": "^GSPC", "etf": "SPY"},
        "NASDAQ": {"idx": "^IXIC", "etf": "QQQ"},
        "RUSSELL 2000": {"idx": "^RUT", "etf": "IWM"}
    }

    for name, tickers in targets.items():
        try:
            # 1. ì§€ìˆ˜ ê°€ê²©
            idx_ticker = yf.Ticker(tickers['idx'])
            hist = idx_ticker.history(period="1d")

            if not hist.empty:
                price = float(hist['Close'].iloc[0])
                d_date = hist.index[0].strftime('%Y-%m-%d')
                collected_data.append((d_date, name, price, 'INDEX_US'))
                print(f"âœ“ {name}: {price:,.2f}")

                # 2. í€ë”ë©˜íƒˆ (ETF ì‚¬ìš©)
                etf_ticker = yf.Ticker(tickers['etf'])
                info = etf_ticker.info

                if 'trailingPE' in info and info['trailingPE']:
                    pe = info['trailingPE']
                    collected_data.append((d_date, f"{name} PER", pe, 'INDEX_US'))

                if 'priceToBook' in info and info['priceToBook']:
                    pbr = info['priceToBook']
                    collected_data.append((d_date, f"{name} PBR", pbr, 'INDEX_US'))

        except Exception as e:
            print(f"âŒ {name} ì˜¤ë¥˜: {e}")

    if collected_data:
        save_to_csv(collected_data)


# ==========================================
# 3. [DRAM/NAND] ë°˜ë„ì²´ ê°€ê²©
# ==========================================
def crawl_dram_nand(data_type):
    """DRAM ë° NAND ê°€ê²© í¬ë¡¤ë§"""
    print(f"\nğŸ“Š {data_type} í¬ë¡¤ë§ ì‹œì‘")
    driver = None
    try:
        driver = setup_driver()
        driver.get(f'https://www.dramexchange.com/#{data_type.lower()}')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'table')))

        current_date = datetime.now().strftime('%Y-%m-%d')
        collected_data = []
        target_items = TARGET_DRAM_ITEMS if data_type == 'DRAM' else TARGET_NAND_ITEMS
        found_items = set()

        tables = driver.find_elements(By.TAG_NAME, 'table')
        for table in tables:
            rows = table.find_elements(By.TAG_NAME, 'tr')
            for row in rows:
                cells = row.find_elements(By.TAG_NAME, 'td')
                if not cells: cells = row.find_elements(By.TAG_NAME, 'th')
                if len(cells) < 2: continue

                item_name = cells[0].text.strip()
                if item_name in target_items and item_name not in found_items:
                    try:
                        price = cells[1].text.strip()
                        if price and price.replace('.', '').replace(',', '').isdigit():
                            val = float(price.replace(',', ''))
                            collected_data.append((current_date, item_name, val, data_type))
                            found_items.add(item_name)
                            print(f"âœ“ {item_name}: ${price}")
                    except:
                        pass

        if collected_data:
            save_to_csv(collected_data)
        else:
            print(f"âš ï¸ {data_type} ë°ì´í„° ì—†ìŒ")

    except Exception as e:
        print(f"âŒ {data_type} ì˜¤ë¥˜: {e}")
    finally:
        if driver: driver.quit()


# ==========================================
# 4. [SCFI] í•´ìƒìš´ì„ì§€ìˆ˜
# ==========================================
def crawl_scfi_index():
    print(f"\nğŸš¢ SCFI í¬ë¡¤ë§ ì‹œì‘")
    driver = None
    try:
        driver = setup_driver()
        driver.get('https://en.sse.net.cn/indices/scfinew.jsp')
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'currdate')))

        scfi_date = driver.find_element(By.ID, 'currdate').text.strip()
        scfi_value = None

        tables = driver.find_elements(By.TAG_NAME, 'table')
        for table in tables:
            if 'Comprehensive Index' in table.text:
                rows = table.find_elements(By.TAG_NAME, 'tr')
                for row in rows:
                    if 'Comprehensive Index' in row.text:
                        idx4 = row.find_elements(By.CSS_SELECTOR, 'span.idx4')
                        if idx4: scfi_value = idx4[0].text.strip()

        if scfi_value and scfi_date:
            if get_last_scfi_date() == scfi_date:
                print(f"ğŸ’¡ SCFI ìµœì‹  ìƒíƒœ ({scfi_date})")
            else:
                save_to_csv([(scfi_date, 'SCFI Comprehensive Index', float(scfi_value), 'OCEAN_FREIGHT')])
                print(f"âœ… SCFI ì €ì¥: {scfi_value}")
    except Exception as e:
        print(f"âŒ SCFI ì˜¤ë¥˜: {e}")
    finally:
        if driver: driver.quit()


# ==========================================
# 5. [yfinance] ê¸°íƒ€ ìì‚°
# ==========================================
def crawl_yfinance_data():
    print(f"\nğŸ“ˆ yfinance í¬ë¡¤ë§ ì‹œì‘")
    current_date = datetime.now().strftime('%Y-%m-%d')
    collected_data = []
    for name, info in YFINANCE_TICKERS.items():
        try:
            t = yf.Ticker(info['ticker'])
            h = t.history(period='1d')
            if not h.empty:
                price = float(h['Close'].iloc[0])
                d = h.index[0].strftime('%Y-%m-%d') if info['type'] != 'CRYPTO' else current_date
                collected_data.append((d, name, price, info['type']))
                print(f"âœ“ {name}: {price:.2f}")
        except:
            print(f"âš ï¸ {name} ì‹¤íŒ¨")

    if collected_data: save_to_csv(collected_data)


# ==========================================
# Main Execution
# ==========================================
def main():
    print("ğŸš€ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
    setup_csv()

    # ìˆœì°¨ì  ì‹¤í–‰
    crawl_dram_nand('DRAM')
    crawl_dram_nand('NAND')
    crawl_scfi_index()
    crawl_yfinance_data()
    crawl_krx_indices()  # FinanceDataReader ë²„ì „
    crawl_us_indices()

    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼: {CSV_FILE}")


if __name__ == "__main__":
    main()
