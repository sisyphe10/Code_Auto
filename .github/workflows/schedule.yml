name: Daily Crypto & RAM Price Crawler

# 1. 실행 조건 설정
on:
  schedule:
    # 매일 한국 시간 밤 11시 (UTC 14:00) 실행
    - cron: '0 14 * * *'
  # 테스트를 위해 수동으로도 실행 가능하게 설정
  workflow_dispatch:

# 2. 권한 설정 (엑셀 파일을 쓰고 저장하기 위해 필수)
permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      # (1) 저장소의 코드를 가져옵니다.
      - name: Checkout code
        uses: actions/checkout@v3

      # (2) 파이썬 3.9 환경을 만듭니다.
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # (3) 필요한 라이브러리를 설치합니다. (requirements.txt 필요)
      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # (4) 작성하신 파이썬 코드를 실행합니다.
      - name: Run crawler code
        run: python main.py

      # (5) 결과물(dataset.xlsx)이 변경되었으면 저장소에 업데이트합니다.
      - name: Commit and Push results
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          
          # 엑셀 파일 추가
          git add dataset.xlsx
          
          # 변경사항이 있으면 커밋하고 푸시, 없으면 넘어감 (에러 방지)
          git commit -m "Auto-update: dataset.xlsx" || echo "No changes to commit"
          git push
